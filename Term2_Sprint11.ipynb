{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習スクラッチ　畳み込みNネットワーク１\n",
    "\n",
    "スクラッチで1次元用畳み込みニューラルネットワークを実装した後、学習と検証を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "#基本ライブラリ\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "#scikit-learnライブラリ\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#データセット\n",
    "#from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNでは画像に対しての2次元畳み込み層が定番だが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装する。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多い。畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されている。\n",
    "\n",
    "検証には引き続きMNISTデータセットを使用する。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n",
      "(48000, 10)\n",
      "(12000, 10)\n"
     ]
    }
   ],
   "source": [
    "#MNISTデータセット\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#平滑化\n",
    "X_train_f = X_train.reshape(-1, 784)\n",
    "X_test_f = X_test.reshape(-1, 784)\n",
    "\n",
    "#前処理\n",
    "X_train_ff = X_train_f.astype(np.float)\n",
    "X_test_ff = X_test_f.astype(np.float)\n",
    "X_train_ff /= 255\n",
    "X_test_ff /= 255\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "Xt_train, Xt_val, yt_train, yt_val = train_test_split(X_train_ff, y_train_one_hot, test_size=0.2)\n",
    "print(Xt_train.shape) # (48000, 784)\n",
    "print(Xt_val.shape) # (12000, 784)\n",
    "print(yt_train.shape)\n",
    "print(yt_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleConv1d\n",
    "\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成する。基本構造は前のSprintで作成した全結合層のFCクラスと同じになる。なお、重みの初期化に関するクラスは必要に応じて作り変える。Xavierの初期値などを使う点は全結合層と同様。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 処理の流れを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 400)\n",
      "(48000, 400)\n",
      "(48000, 400)\n",
      "[0.95238779 0.15849382 0.80350779]\n"
     ]
    }
   ],
   "source": [
    "#重み\n",
    "n_features = 784\n",
    "n_nodes1 = 400\n",
    "sigma = 0.01 # ガウス分布の標準偏差\n",
    "W1 = sigma * np.random.randn(n_features, n_nodes1) #（n行、m列）でガウス分布を作る\n",
    "print(W1.shape)\n",
    "\n",
    "#バイアス\n",
    "n_features = 1\n",
    "n_nodes1 = 400\n",
    "sigma = 0.01 # ガウス分布の標準偏差\n",
    "B1 = sigma * np.random.randn(n_features, n_nodes1) #（n行、m列）でガウス分布を作る\n",
    "\n",
    "#XW　＝48,000×400の行列を作る\n",
    "XW1 = np.dot(Xt_train, W1)\n",
    "\n",
    "print(XW1.shape)  #(48000, 400)\n",
    "\n",
    "A1 = XW1+B1\n",
    "\n",
    "print(A1.shape)\n",
    "\n",
    "filter1 = np.random.rand(3)\n",
    "print(filter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA1 = np.zeros(A1.shape)\n",
    "\n",
    "for i in range(0, A1.shape[1] - len(filter1) + 1):\n",
    "    FA1[0][i] = np.sum(A1[0][i: i+len(filter1)]*filter1)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35 50]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "a = np.array([0,0])\n",
    "\n",
    "for i in range(0, len(x) - len(w) + 1):\n",
    "    a[i] = np.sum(x[i: i+len(w)]*w)+b\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35 50]\n"
     ]
    }
   ],
   "source": [
    "#実装上の工夫\n",
    "indexes = np.array([[0,1,2],[1,2,3]])\n",
    "print(np.dot(x[indexes], w)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力サイズの関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def N_out(N_in, P, F, S):\n",
    "    return (N_in + 2*P - F) / S + 1\n",
    "\n",
    "P = 0\n",
    "F = len(w)\n",
    "S = 1\n",
    "\n",
    "print(N_out(len(x), P, F, S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バックプロパゲーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([45, 70])\n",
    "da = y - a\n",
    "print(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "バイアスの傾き: 30\n"
     ]
    }
   ],
   "source": [
    "db = np.sum(da)\n",
    "print(\"バイアスの傾き:\", db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フィルタの傾き [ 50.  80. 110.]\n"
     ]
    }
   ],
   "source": [
    "dw = np.zeros(3)\n",
    "for i in range(2):\n",
    "    dw += (x[i: i+len(w)] * da[i])\n",
    "print(\"フィルタの傾き\", dw)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xの傾き: [ 30. 110. 170. 140.]\n"
     ]
    }
   ],
   "source": [
    "dx = np.zeros(len(x))\n",
    "for i in range(2):\n",
    "    dx[i: i+len(w)] += da[i]*w\n",
    "print(\"xの傾き:\", dx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initializer, optimizer, pad=0, fsize=0, stride=1):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.pad = pad\n",
    "        self.fsize = fsize\n",
    "        self.stride = stride\n",
    "        self.W =  initializer.W(fsize)\n",
    "        self.B = 0\n",
    "  \n",
    "    \n",
    "    def forward(self, X, w, b, init=False):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        if init:\n",
    "            self.X = X\n",
    "            self.W = w\n",
    "            self.B = b\n",
    "            self.LW = np.zeros(len(self.W))\n",
    "            self.LX = np.zeros(len(X))\n",
    "            self.A = np.zeros(int(N_out(len(X), self.pad, self.fsize, self.stride)))\n",
    "        \n",
    "        for i in range(0, len(X) - len(self.W) + 1):\n",
    "            self.A[i] = np.sum(X[i: i+len(self.W)] * self.W)+self.B        \n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.LB = np.sum(dA)\n",
    "        \n",
    "        for i in range(len(dA)):\n",
    "            self.LW += (self.X[i: i+len(self.W)] * dA[i])\n",
    "        \n",
    "        for i in range(2):\n",
    "            self.LX[i: i+len(self.W)] += dA[i]*self.W\n",
    "        \n",
    "        LX = self.LX\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        #FC2 = FC2.optimizer.updata(FC2)\n",
    "        #FC2 = SGD().update(FC2)\n",
    "        \n",
    "        return LX\n",
    "    \n",
    "    def N_out(N_in, P, F, S):\n",
    "        return (N_in + 2*P - F) / S + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [35. 50.]\n",
      "dZ: [ 30. 110. 170. 140.]\n",
      "dW: [ 50.  80. 110.]\n",
      "dB: 30\n"
     ]
    }
   ],
   "source": [
    "test = SimpleConv1d(OneInit(), SGD(), pad=0, fsize=3, stride=1)\n",
    "\n",
    "x = np.array([1.0,2.0,3.0,4.0])\n",
    "w = np.array([3.0, 5.0, 7.0])\n",
    "b = np.array([1.0])\n",
    "\n",
    "A = test.forward(x, w, b, init=True)\n",
    "print(\"A:\", A)\n",
    "\n",
    "dZ = test.backward(da)\n",
    "print(\"dZ:\", dZ)\n",
    "print(\"dW:\", test.LW)\n",
    "print(\"dB:\",  test.LB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1d（複数チャネル）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "   \n",
    "    def __init__(self, initializer, optimizer, fsize, in_channel=1, out_channel=1, pad=0, stride=1):\n",
    "        self.optimizer = optimizer\n",
    "        self.initializer = initializer\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.pad = pad\n",
    "        self.fsize = fsize   #1次元\n",
    "        self.stride = stride\n",
    "        self.B = 0\n",
    "    \n",
    "    def forward(self, X, init=False):\n",
    "\n",
    "        if init:\n",
    "            self.X = self.pad_init(X, self.pad)\n",
    "            self.W =  self.initializer.W(self.fsize, self.in_channel, self.out_channel)\n",
    "            self.B = self.initializer.B(self.out_channel)\n",
    "            self.LW = np.zeros(self.W.shape)\n",
    "            self.dZ = np.zeros(self.X.shape)\n",
    "            self.A = np.zeros((self.out_channel, int(self.N_out(X.shape[1], self.pad, self.fsize, self.stride))))\n",
    "        \n",
    "        for k in range(self.out_channel):\n",
    "            for j in range(self.X.shape[0]):\n",
    "                for i in range(0, self.X.shape[1] - self.W.shape[2] + 1):\n",
    "                    self.A[k, i] += np.sum(self.X[j, i: i+self.W.shape[2]] * self.W[k, j])\n",
    "            self.A[k] += self.B[k]\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "\n",
    "        self.LB = np.sum(dA, axis=1)\n",
    "        \n",
    "        for k in range(self.out_channel):\n",
    "            for j in range(self.X.shape[0]):\n",
    "                for i in range(dA.shape[1]):\n",
    "                    self.LW[k, i] += (self.X[j, i: i+self.W.shape[2]] * dA[k, j])\n",
    "\n",
    "        for k in range(self.out_channel):\n",
    "            for j in range(self.X.shape[0]):\n",
    "                for i in range(dA.shape[1]):\n",
    "                    print(dA[k, i]*self.W[k, j])\n",
    "                    self.dZ[j, i: i+self.W.shape[2]] += dA[k, i]*self.W[k, j]\n",
    "                    \n",
    "        dZ = self.dZ\n",
    "        \n",
    "        # 更新\n",
    "        self.optimizer.update(self)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def N_out(self, N_in, P, F, S):\n",
    "        return (N_in + 2*P - F) / S + 1\n",
    "    \n",
    "    def pad_init(self, X, pad):\n",
    "        \"\"\"\n",
    "        １次元方向にパディングを追加する関数\n",
    "        \"\"\"\n",
    "        for i in range(pad):\n",
    "            X = np.insert(X, 0, 0, axis=1)\n",
    "            X = np.insert(X, X.shape[1], 0, axis=1)\n",
    "        \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n",
      "B [1. 2. 3.]\n",
      "W [[[1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "test = Conv1d(OneInit(), SGD(), fsize=3, in_channel=2, out_channel=3, pad=0, stride=1)\n",
    "\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])\n",
    "\n",
    "A = test.forward(x, init=True)\n",
    "print(\"A:\", A)\n",
    "print(\"B\", test.B)\n",
    "print(\"W\", test.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29. 48.]\n",
      " [28. 47.]\n",
      " [27. 46.]]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([45, 70])\n",
    "da = y - A\n",
    "print(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29. 29. 29.]\n",
      "[48. 48. 48.]\n",
      "[29. 29. 29.]\n",
      "[48. 48. 48.]\n",
      "[28. 28. 28.]\n",
      "[47. 47. 47.]\n",
      "[28. 28. 28.]\n",
      "[47. 47. 47.]\n",
      "[27. 27. 27.]\n",
      "[46. 46. 46.]\n",
      "[27. 27. 27.]\n",
      "[46. 46. 46.]\n",
      "-------\n",
      "dZ: [[ 84. 225. 225. 141.]\n",
      " [ 84. 225. 225. 141.]]\n",
      "dW: [[[125. 202. 279.]\n",
      "  [202. 279. 356.]]\n",
      "\n",
      " [[122. 197. 272.]\n",
      "  [197. 272. 347.]]\n",
      "\n",
      " [[119. 192. 265.]\n",
      "  [192. 265. 338.]]]\n",
      "dB: [77. 75. 73.]\n",
      "NewB: [0.23 1.25 2.27]\n",
      "NewW: [[[-0.25 -1.02 -1.79]\n",
      "  [-1.02 -1.79 -2.56]]\n",
      "\n",
      " [[-0.22 -0.97 -1.72]\n",
      "  [-0.97 -1.72 -2.47]]\n",
      "\n",
      " [[-0.19 -0.92 -1.65]\n",
      "  [-0.92 -1.65 -2.38]]]\n"
     ]
    }
   ],
   "source": [
    "dZ = test.backward(da)\n",
    "print(\"-------\")\n",
    "print(\"dZ:\", dZ)\n",
    "print(\"dW:\", test.LW)\n",
    "print(\"dB:\",  test.LB)\n",
    "print(\"NewB:\", test.B)\n",
    "print(\"NewW:\", test.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     22
    ]
   },
   "outputs": [],
   "source": [
    "#SGD for Conv\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr*layer.LW\n",
    "        layer.B -= self.lr*layer.LB\n",
    "\n",
    "#AdaGrad for NN\n",
    "class AdaGrad():\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.hw = 0\n",
    "        self.hb = 0\n",
    "        self.hw_mean=[]\n",
    "        self.hb_mean=[]\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        self.hw += (layer.LW)**2 \n",
    "        self.hb +=  (layer.LB)**2\n",
    "        self.hw_mean.append(np.mean(self.hw))\n",
    "        self.hb_mean.append(np.mean(self.hb))\n",
    "        \n",
    "        layer.W -= self.lr / np.sqrt(self.hw + 1e-7) * layer.LW\n",
    "        layer.B -= self.lr / np.sqrt(self.hb + 1e-7) * layer.LB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneInit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#ザビエル\n",
    "class OneInit():\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, w, input_channel=1, output_channel=1 ):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = np.ones((output_channel, input_channel, w))\n",
    "\n",
    "        return W\n",
    "    \n",
    "    def B(self, output_channel):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.arange(1, output_channel + 1) .astype(float)\n",
    "        \n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavierl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#ザビエル for Conv\n",
    "class XavierlInit():\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, w, input_channel=1, output_channel=1 ):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        xavier = 1 / np.sqrt(w)\n",
    "        W = xavier * np.random.randn((output_channel, input_channel, w))\n",
    "\n",
    "        return W\n",
    "    \n",
    "    def B(self, output_channel):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, output_channel) \n",
    "        \n",
    "        return B\n",
    "\n",
    "#ザビエル for NN\n",
    "class XavierInitializer():\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        xavier = 1 / np.sqrt(n_nodes1)\n",
    "        W = xavier * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2) \n",
    "        \n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#フー\n",
    "class HeInitializer():\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        he = np.sqrt( 2 / n_nodes1 )\n",
    "        W = he * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2) \n",
    "        \n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任意のストライド数\n",
    "\n",
    "ストライドを1から2や3に増やすことで、大雑把に畳み込み演算することになるため、出力サイズが小さくなり、処理が短時間になる一方で、画像の特徴を取りこぼす可能性がある。そのため畳み込み層ではストライドはあまり使われない。 よっぽどサイズが大きい画像に特徴抽出したい対象が大きく写っている場合は、ストライドを適用しても特徴を失うことなく効率的に処理ができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "##NNに必要なクラス\n",
    "class relu():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.mask = (X <= 0)\n",
    "        self.mask = self.mask.reshape(1,-1)\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, LZ, A):\n",
    "        LZ[self.mask] = 0\n",
    "        dx = LZ\n",
    "        return dx \n",
    "\n",
    "class Tanh():\n",
    "    \"\"\"\n",
    "    ハイパボリックタンジェント関数\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        return np.tanh(X)\n",
    "    \n",
    "    def backward(self, LZ, A):\n",
    "        return LZ * (1-np.tanh(A)**2) \n",
    "\n",
    "class Softmax():\n",
    "    \"\"\"\n",
    "    ソフトマックス関数\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        c = np.max(X, axis=1)\n",
    "        exp_x = np.exp(X - c.reshape(-1,1))   #オーバーフロー対策\n",
    "        sum_exp_x = np.sum(exp_x, axis=1)\n",
    "        y = exp_x / sum_exp_x.reshape(-1,1)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def backward(self, X):\n",
    "        print(\"Softmax backward isn't made yet\")\n",
    "        pass\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, def_name=None):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W =  initializer.W(n_nodes1, n_nodes2) \n",
    "        self.B = initializer.B(n_nodes2) \n",
    "  \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        XW = np.dot(X, self.W)\n",
    "        self.A = XW + self.B\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA, Z, FC_num):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        if FC_num == 0:\n",
    "            self.LB = np.sum(dA, axis=0)\n",
    "            self.LW = np.dot(Z.T, self.LA)\n",
    "            dZ = np.dot(self.LA, self.W.T)\n",
    "        else:\n",
    "            self.LB = np.sum(dA, axis=0)\n",
    "            self.LW = np.dot(Z.T, self.LA)\n",
    "            dZ = np.dot(self.LA, self.W.T)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        #FC2 = FC2.optimizer.updata(FC2)\n",
    "        #FC2 = SGD().update(FC2)\n",
    "        \n",
    "        return dZ\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練用データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    多層ニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nodes_num_list, ac_list, initialize_list, opt_list,\n",
    "                 itr=1000, verbose=True, lr=0.00001, sigma=0.01, b_size=20):\n",
    "        self.verbose = verbose\n",
    "        self.itr = itr\n",
    "        self.lr = lr                                     #学習率\n",
    "        self.sigma = sigma                       #ガウス分布の標準偏差\n",
    "        self.b_size = b_size\n",
    "        self.L = []\n",
    "        self.FC = {}                                  #各レイヤー格納辞書\n",
    "        self.network = len(nodes_num_list)\n",
    "        self.n_nodes = {i: nodes_num_list[i] for i in range(self.network)}\n",
    "        self.activation = {i: ac_list[i] for i in range(self.network)}\n",
    "        self.initialize_list = initialize_list\n",
    "        for i in range(len(self.initialize_list)):\n",
    "            self.initialize_list[i].sigma = self.sigma\n",
    "        self.opt_list = opt_list\n",
    "        for i in range(len(self.opt_list)):\n",
    "            self.opt_list[i].lr = self.lr\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "        #各レイヤーインスタンス化 \n",
    "        Z, LZ = [],[]\n",
    "        for i in range(self.network):\n",
    "            if i == 0:\n",
    "                self.FC[i] = FC(X.shape[1], self.n_nodes[i], self.initialize_list[i], self.opt_list[i])\n",
    "            else:\n",
    "                self.FC[i] = FC(self.n_nodes[i-1], self.n_nodes[i], self.initialize_list[i], self.opt_list[i])\n",
    "            Z.append(np.array([]))\n",
    "            LZ.append(np.array([]))\n",
    "        \n",
    "        #エポック\n",
    "        itr_count = 0\n",
    "        for _ in range(self.itr):\n",
    "            if itr_count%10 == 0:\n",
    "                print(itr_count,\">\", end=\"\")\n",
    "            Lbatch = np.array([])\n",
    "            \n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.b_size)\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "            \n",
    "                \"\"\"フォワード\n",
    "                \"\"\"\n",
    "                for i in range(self.network):\n",
    "                    if i == 0:\n",
    "                        A = self.FC[i].forward(mini_X_train)\n",
    "                        Z[i] = self.activation[i].forward(A)\n",
    "                    else:\n",
    "                        A = self.FC[i].forward(Z[i-1])\n",
    "                        Z[i] = self.activation[i].forward(A)\n",
    "\n",
    "                \"\"\"バックプロパゲーション\n",
    "                \"\"\"\n",
    "                for i in range(self.network)[::-1]:\n",
    "                    if i == self.network-1:  #出力層\n",
    "                        self.FC[i].LA = Z[i] - mini_y_train\n",
    "                        LZ[i] = self.FC[i].backward(self.FC[i].LA, Z[i-1], i)\n",
    "                    elif i == 0:  #入力層\n",
    "                        self.FC[i].LA = self.activation[i].backward(LZ[i+1], self.FC[i].A)\n",
    "                        LZ[i] = self.FC[i].backward(self.FC[i].LA, mini_X_train, i)\n",
    "                    else:  #隠れ層\n",
    "                        self.FC[i].LA = self.activation[i].backward(LZ[i+1], self.FC[i].A)\n",
    "                        LZ[i] = self.FC[i].backward(self.FC[i].LA, Z[i-1], i)\n",
    "\n",
    "                \"\"\"損失関数\n",
    "                \"\"\"\n",
    "                Ltmp = 0\n",
    "                for i in range(self.b_size):\n",
    "                    Ltmp += np.sum(mini_y_train[i] * np.log(Z[-1][i]))\n",
    "                Lbatch = np.append(Lbatch, (Ltmp/ self.b_size) * -1)\n",
    "                \n",
    "            self.L.append(Lbatch.mean())    \n",
    "            itr_count += 1\n",
    "            #print(\"---end of epoc---\")\n",
    "\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            print()\n",
    "        pass\n",
    "\n",
    "    def predict(self, Xt):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        pZ = []\n",
    "        for i in range(self.network):\n",
    "            pZ.append(np.array([]))        \n",
    "\n",
    "        for i in range(self.network):\n",
    "            if i == 0:\n",
    "                A = self.FC[i].forward(Xt)\n",
    "                pZ[i] = self.activation[i].forward(A)\n",
    "            else:\n",
    "                A = self.FC[i].forward(pZ[i-1])\n",
    "                pZ[i] = self.activation[i].forward(A)\n",
    "                    \n",
    "        return pZ[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_num_list = [100, 400, 200, 100, 10]\n",
    "ac_list =               [relu(), relu(), relu(), relu(), Softmax()]\n",
    "initialize_list =       [HeInitializer(), HeInitializer(), HeInitializer(),  HeInitializer(), XavierInitializer()]\n",
    "opt_list =              [AdaGrad(), AdaGrad(), AdaGrad(), AdaGrad(), AdaGrad()]\n",
    "\n",
    "SDNN = ScratchDeepNeuralNetrowkClassifier(\n",
    "    nodes_num_list, ac_list, initialize_list, opt_list, itr=20, b_size=100, lr=0.01)\n",
    "#SDNN.fit(Xt_train, yt_train)\n",
    "#plt.plot(SDNN.L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch1dCNNClassifier\n",
    "\n",
    "隠れ層２層のうち、前の１層をConvにしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Conv1d:\n",
    "    \n",
    "    def __init__(self, initializer, optimizer, fsize, in_channel=1, out_channel=1, pad=0, stride=1):\n",
    "        self.optimizer = optimizer\n",
    "        self.initializer = initializer\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.pad = pad\n",
    "        self.fsize = fsize   #1次元\n",
    "        self.stride = stride\n",
    "        self.B = 0\n",
    "    \n",
    "    def init(self, X):\n",
    "        \"\"\"\n",
    "        Xは二次元を想定\n",
    "        \"\"\"\n",
    "        self.X = self.pad_init(X, self.pad)\n",
    "        self.W =  self.initializer.W(1, self.fsize)\n",
    "        self.B = self.initializer.B(1)\n",
    "        self.LB = np.zeros(self.B.shape)\n",
    "        self.LW = np.zeros(self.W.shape)\n",
    "        self.dZ = np.zeros(self.X.shape)\n",
    "        self.A = np.zeros((X.shape[0], int(self.N_out(X.shape[1], self.pad, self.fsize, self.stride))))\n",
    "        self.LA = np.copy(self.A)\n",
    "\n",
    "    def forward(self, X, index):\n",
    "        \"\"\"\n",
    "        Xは１次元を想定\n",
    "        \"\"\"\n",
    "        for i in range(0, len(X) - self.W.shape[1] + 1):\n",
    "            self.A[index, i] = np.sum(X[i: i+self.W.shape[1]] * self.W)+self.B        \n",
    "        \n",
    "        return self.A[index]\n",
    "    \n",
    "    def backward(self, dA, index):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.LB = np.sum(dA)\n",
    "        \n",
    "        for i in range(len(dA)):\n",
    "            self.LW += (self.X[index, i: i+self.W.shape[1]] * dA[i])\n",
    "        \n",
    "        for i in range(len(dA)):\n",
    "            self.dZ[index, i: i + self.W.shape[1]] += dA[i]*self.W.flatten()\n",
    "        \n",
    "        dZ = self.dZ\n",
    "        \n",
    "        self = self.optimizer.update(self, index)\n",
    "\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def N_out(self, N_in, P, F, S):\n",
    "        return (N_in + 2*P - F) / S + 1\n",
    "\n",
    "    def pad_init(self, X, pad):\n",
    "        \"\"\"\n",
    "        １次元方向にパディングを追加する関数\n",
    "        \"\"\"\n",
    "        for i in range(pad):\n",
    "            X = np.insert(X, 0, 0, axis=1)\n",
    "            X = np.insert(X, X.shape[1], 0, axis=1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "class NN_SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer, index):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr*layer.LW\n",
    "        layer.B -= self.lr*layer.LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch1dCNNClassifier():\n",
    "    \"\"\"\n",
    "    多層ニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nodes_num_list, ac_list, initialize_list, opt_list,\n",
    "                 itr=1000, verbose=True, lr=0.00001, sigma=0.01, b_size=20):\n",
    "        self.verbose = verbose\n",
    "        self.itr = itr\n",
    "        self.lr = lr                                     #学習率\n",
    "        self.sigma = sigma                       #ガウス分布の標準偏差\n",
    "        self.b_size = b_size\n",
    "        self.L = []\n",
    "        self.FC = {}                                  #各レイヤー格納辞書\n",
    "        self.network = len(nodes_num_list)\n",
    "        self.n_nodes = {i: nodes_num_list[i] for i in range(self.network)}\n",
    "        self.activation = {i: ac_list[i] for i in range(self.network)}\n",
    "        self.initialize_list = initialize_list\n",
    "        for i in range(len(self.initialize_list)):\n",
    "            self.initialize_list[i].sigma = self.sigma\n",
    "        self.opt_list = opt_list\n",
    "        for i in range(len(self.opt_list)):\n",
    "            self.opt_list[i].lr = self.lr\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "        #各レイヤーインスタンス化 \n",
    "        Z, LZ = [],[]\n",
    "        for i in range(self.network):\n",
    "            if i == 0:\n",
    "                self.FC[i] = NN_Conv1d(self.initialize_list[i], self.opt_list[i], fsize=385, in_channel=1, out_channel=1, pad=0)\n",
    "            else:\n",
    "                self.FC[i] = FC(self.n_nodes[i-1], self.n_nodes[i], self.initialize_list[i], self.opt_list[i])\n",
    "            Z.append(np.array([]))\n",
    "            LZ.append(np.array([]))\n",
    "        \n",
    "        #エポック\n",
    "        itr_count = 0\n",
    "        self.FC[0].init(X)\n",
    "        for _ in range(self.itr):\n",
    "            if itr_count%2 == 0:\n",
    "                print(itr_count,\">\", end=\"\")\n",
    "            Lbatch = np.array([])\n",
    "            index = 0\n",
    "            \n",
    "            for mini_X_train, mini_y_train in zip(X, y):\n",
    "            \n",
    "                \"\"\"フォワード\n",
    "                \"\"\"\n",
    "                for i in range(self.network):\n",
    "                    if i == 0:\n",
    "                        A = self.FC[i].forward(mini_X_train, index)\n",
    "                        Z[i] = self.activation[i].forward(A).reshape(1,400)\n",
    "                        \n",
    "                    else:\n",
    "                        A = self.FC[i].forward(Z[i-1])\n",
    "                        Z[i] = self.activation[i].forward(A)\n",
    "                    \n",
    "                \"\"\"バックプロパゲーション\n",
    "                \"\"\"\n",
    "                for i in range(self.network)[::-1]:\n",
    "                    if i == self.network-1:  #出力層\n",
    "                        self.FC[i].LA = Z[i] - mini_y_train\n",
    "                        LZ[i] = self.FC[i].backward(self.FC[i].LA, Z[i-1], i)\n",
    "                    elif i == 0:  #Conv層\n",
    "                        self.FC[i].LA[index] = self.activation[i].backward(LZ[i+1], self.FC[i].A[index])\n",
    "                        LZ[i] = self.FC[i].backward(self.FC[i].LA[index], index)\n",
    "                    else:  #隠れ層\n",
    "                        self.FC[i].LA = self.activation[i].backward(LZ[i+1], self.FC[i].A)\n",
    "                        LZ[i] = self.FC[i].backward(self.FC[i].LA, Z[i-1], i)\n",
    "\n",
    "                \"\"\"損失関数\n",
    "                \"\"\"\n",
    "                Ltmp = 0\n",
    "                for i in range(self.b_size):\n",
    "                    Ltmp += np.sum(mini_y_train[i] * np.log(Z[-1][i]))\n",
    "                Lbatch = np.append(Lbatch, (Ltmp/ self.b_size) * -1)\n",
    "                index += 1\n",
    "                \n",
    "            self.L.append(Lbatch.mean())    \n",
    "            itr_count += 1\n",
    "            #print(\"---end of epoc---\")\n",
    "\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            print()\n",
    "        pass\n",
    "\n",
    "    def predict(self, Xt):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        pZ = []\n",
    "        for i in range(self.network):\n",
    "            pZ.append(np.array([]))        \n",
    "\n",
    "        index = 0\n",
    "        for mini_X in Xt:\n",
    "            for i in range(self.network):\n",
    "                if i == 0:\n",
    "                    A = self.FC[i].forward(Xt, index)\n",
    "                    pZ[i] = self.activation[i].forward(A).reshape(1,400)\n",
    "                elif i == 2:\n",
    "                    A = self.FC[i].forward(pZ[i-1])\n",
    "                    pZ[i] = np.append(pZ[i], self.activation[i].forward(A))\n",
    "                else:\n",
    "                    A = self.FC[i].forward(pZ[i-1])\n",
    "                    pZ[i] = self.activation[i].forward(A)\n",
    "                    \n",
    "                    \n",
    "            index += 1\n",
    "                    \n",
    "        return pZ[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 >2 >4 >\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1098dfeb8>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xt_train1d = Xt_train[0:5000]\n",
    "yt_train1d = yt_train[0:5000]\n",
    "\n",
    "nodes_num_list = [400, 200, 10]\n",
    "ac_list =               [relu(), relu(), Softmax()]\n",
    "initialize_list =       [XavierInitializer(),  HeInitializer(), XavierInitializer()]\n",
    "opt_list =              [NN_SGD(), AdaGrad(), AdaGrad()]\n",
    "\n",
    "CNN = Scratch1dCNNClassifier(\n",
    "    nodes_num_list, ac_list, initialize_list, opt_list, itr=5, b_size=1, lr=0.0001)\n",
    "CNN.fit(Xt_train1d, yt_train1d)\n",
    "plt.plot(CNN.L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 385)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN.FC[0].W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2817899123131857]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN.L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_pred = np.array([])\\nfor i in y_pred_raw:\\n    y_pred = np.append(y_pred, np.argmax(i))\\n\\nprint(y_pred.shape)\\nprint(y_pred[0:5])\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt_test1d = Xt_val[0:1000]\n",
    "yt_test1d = yt_val[0:1000]\n",
    "\n",
    "#y_pred_raw = CNN.predict(Xt_test1d)\n",
    "#print(y_pred_raw.shape)\n",
    "\"\"\"\n",
    "y_pred = np.array([])\n",
    "for i in y_pred_raw:\n",
    "    y_pred = np.append(y_pred, np.argmax(i))\n",
    "\n",
    "print(y_pred.shape)\n",
    "print(y_pred[0:5])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, {1: 20, 2: 30})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jisyo = {1:12, 2:30}\n",
    "taple = (1,2,3,jisyo)\n",
    "taple[3][1] = 20\n",
    "taple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risuto = [1,2,3,4]\n",
    "risuto = risuto + [5]\n",
    "risuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
