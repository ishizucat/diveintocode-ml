{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras入門\n",
    "\n",
    "## Kerasとは\n",
    "\n",
    "Keras は元々はTheano、TensorFLowなど複数のフレームワークを扱いやすくする ラッパー として登場したライブラリでしたが、後にTensorFLowの 高レベルAPI として使われるようになりました。\n",
    "TensorFLowに含まれる形のKerasであるtf.kerasを主に使っていきます。\n",
    "\n",
    "《ラッパーとは》\n",
    "ラッパーはもともとのプログラムの機能を利用して、より使いやすいものを提供します。TensorFlowはニューラルネットワークに必要な計算を効率的に行う機能を提供しますが、初期のころはモデルを構築して学習を行うとなると手間がかかる部分もありました。そのため、TensorFlowをラップして、扱いやすくするKerasが登場しました。\n",
    "\n",
    "《高レベルAPIとは》\n",
    "大きな単位で機能を簡単に扱えるように作られたものが高レベルAPIです。対義語として、細かい単位で機能をいじれるが、扱いがその分大変な低レベルAPIがあります。\n",
    "TensorFlow自体でもニューラルネットワークのモデル構築や学習を行いやすくするために、高レベルAPIの充実が進められています。tf.Kerasはそのひとつです。\n",
    "\n",
    "Kerasでは簡素にニューラルネットワークが記述できます。その書き方にはSequentialモデルとFunctional APIの2種類があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kerasでロジスティック回帰によるANDゲートを作成\n",
    "\n",
    "import numpy as np\n",
    "# ANDゲートの学習データを用意\n",
    "x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_train = np.array([[0],[0],[0],[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequentialクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ishizucat/.pyenv/versions/anaconda3-5.3.1/envs/term3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Sequentialクラスを使用して全結合層を作成。\n",
    "\n",
    "import tensorflow as tf     #1.13.1の仮想環境\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1,                 #出力のユニット数\n",
    "                                                   activation = tf.nn.sigmoid,      #活性化関数\n",
    "                                                   input_shape=(2,))]                  #入力のユニット数\n",
    "                           )\n",
    "\n",
    "#Denseクラスは引数で重みの初期化方法、バイアスの有無などの指定も可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#サマリ\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "構造が記述できたら、モデルをコンパイルします。コンパイル時に損失関数と最適化手法、評価関数を指定します。損失関数は名前をstringで指定します。ここでは2値分類のため、binary_crossentropyとなります。多値分類の場合はcategorical_crossentropy、回帰の場合はmean_squared_errorのようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "                      metrics=[\"accuracy\"]\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を行います。scikit-learn同様にfitメソッドを使う設計になっています。verboseは学習過程の可視化方法のパラメータで、デフォルトの1ではバッチごとに更新されるプログレスバーが表示されます。verboseが0の場合は表示を行わず、2の場合はエポック毎の表示になります。検証用データがある場合は、引数validation_dataに与えることで、エポック毎の検証も可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ishizucat/.pyenv/versions/anaconda3-5.3.1/envs/term3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "history  = model.fit(x_train, y_train, batch_size=1, epochs=1000, verbose=0\n",
    "                    #validation_data=(x_train, y_train)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_proba [0.00231579 0.10412949 0.10567778 0.85543257]\n",
      "y_pred [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "#推定\n",
    "y_pred_proba = model.predict(x_train)[:, 0]\n",
    "\n",
    "#確率を０、１に変換\n",
    "y_pred = np.where(y_pred_proba > 0.5, 1, 0)\n",
    "\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09502875059843063\n",
      "Train accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#評価のみ行う場合\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addメソッドを使っても記述できる\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid, input_shape=(2,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 複数層の場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#２層ニューラルネットの場合\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addメソッド\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape=(2,)))\n",
    "model.add(tf.keras.layers.Dense(1, activation = tf.nn.sigmoid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API\n",
    "\n",
    "Functional APIを使えばより自由度の高いモデル構築が行えます。Sequentialクラスの代わりにModelクラスを使用します。\n",
    "モデルの記述以降はSequentialと同じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "#入力から出力までの流れを記述\n",
    "input_data = tf.keras.layers.Input(shape=(2,))\n",
    "output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(input_data)\n",
    "\n",
    "#Modelクラスに渡す\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=1000,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 複数層の場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#４層\n",
    "input_data = tf.keras.layers.Input(shape=(2,))\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#枝分かれの表現。３層目で２つに枝分かれし、次の層で結合している。\n",
    "input_data = tf.keras.layers.Input(shape=(2,))\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "y1 = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "y2 = tf.keras.layers.Dense(10, activation=tf.nn.relu)(x)\n",
    "z = tf.keras.layers.concatenate([y1, y2])\n",
    "output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(z)\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ラッパーとしてのKeras\n",
    "\n",
    "ラッパーとしてのKerasもデフォルトでTensorFlowをバックエンドとして使用しているため、基本的な使い方は同じです。\n",
    "\n",
    "以下のコードのほとんどは上で紹介したtf.kerasと実質的に同じですが、例えば活性化関数を全結合層とは別のクラスとして渡しています。また、最適化手法の部分はtf.train.AdamOptimizerからkeras.optimizers.Adamに変わっています。tf.kerasではTensorFlow自体の最適化手法クラスを呼んでいるのに対し、KerasではKeras独自の最適化手法クラスを使用するためです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(2,)))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=Adam(lr=0.01),\n",
    "              metrics=[\"accuracy\"]\n",
    "             )\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=1, epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ディープラーニングフレームワーク２\n",
    "\n",
    "前半はTensorFlowのExampleを動かします。後半ではKerasのコードを書いていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 公式Example\n",
    "\n",
    "深層学習フレームワークには公式に様々なモデルのExampleコードが公開されています。\n",
    "\n",
    "**research**\n",
    "定番のモデルから最新のモデルまで多様なコードが公開されています。\n",
    "\n",
    "https://github.com/tensorflow/models/tree/master/research\n",
    "\n",
    "**tutorials**\n",
    "TensorFLowのチュートリアルとして用意された簡単なモデルが含まれています。\n",
    "\n",
    "https://github.com/tensorflow/models/tree/master/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorialsのcifar10_estimaterを実行してみる\n",
    "\n",
    "＜URL＞\n",
    "https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは、Prerequisiteより、準備を整える\n",
    "\n",
    "1. Install TensorFlow version 1.9.0 or later.\n",
    "　**→Done**\n",
    "2. Download the CIFAR-10 dataset and generate TFRecord files using the provided script. The script and associated command below will download the CIFAR-10 dataset and then generate a TFRecord for the training, validation, and evaluation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and extract.\n",
      "Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n",
      "Generating ../cifar/cifar10_estimator/cifar-10-data/train.tfrecords\n",
      "Generating ../cifar/cifar10_estimator/cifar-10-data/validation.tfrecords\n",
      "Generating ../cifar/cifar10_estimator/cifar-10-data/eval.tfrecords\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%run ../cifar/cifar10_estimator/generate_cifar10_tfrecords.py --data-dir=../cifar/cifar10_estimator/cifar-10-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the command above, you should see the following files in the --data-dir (ls -R cifar-10-data):\n",
    "\n",
    "- train.tfrecords\n",
    "- validation.tfrecords\n",
    "- eval.tfrecords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on a single machine with GPUs or CPU\n",
    "\n",
    "Run the training on CPU only. After training, it runs the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ishizucat/git/cifar/cifar10_estimator/cifar10_main.py:366: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "WARNING:tensorflow:From /Users/ishizucat/git/cifar/cifar10_estimator/cifar10_main.py:373: run (from tensorflow.contrib.learn.python.learn.learn_runner) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.estimator.train_and_evaluate.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a518a2e80>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {\n",
      "  force_gpu_compatible: true\n",
      "}\n",
      "allow_soft_placement: true\n",
      ", '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/cifar10'}\n",
      "WARNING:tensorflow:From /Users/ishizucat/git/cifar/cifar10_estimator/cifar10_main.py:346: Experiment.__init__ (from tensorflow.contrib.learn.python.learn.experiment) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.estimator.train_and_evaluate. You will also have to convert to a tf.estimator.Estimator.\n",
      "WARNING:tensorflow:From /Users/ishizucat/.pyenv/versions/anaconda3-5.3.1/envs/term3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:279: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /Users/ishizucat/git/cifar/cifar10_estimator/model_base.py:174: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_2/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_3/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_4/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_5/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_6/: (?, 32, 32, 16)\n",
      "WARNING:tensorflow:From /Users/ishizucat/git/cifar/cifar10_estimator/model_base.py:204: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.average_pooling2d instead.\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/avg_pool/: (?, 16, 16, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_5/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_6/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/avg_pool/: (?, 8, 8, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_2/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_3/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_4/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_5/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_6/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/global_avg_pool/: (?, 64)\n",
      "WARNING:tensorflow:From /Users/ishizucat/git/cifar/cifar10_estimator/model_base.py:196: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "INFO:tensorflow:image after unit resnet/tower_0/fully_connected/: (?, 11)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-10T06:34:14Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /Users/ishizucat/.pyenv/versions/anaconda3-5.3.1/envs/term3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10/model.ckpt-5\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-10-06:52:31\n",
      "INFO:tensorflow:Saving dict for global step 5: accuracy = 0.1, global_step = 5, loss = 56237010.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5: /tmp/cifar10/model.ckpt-5\n"
     ]
    }
   ],
   "source": [
    "%run ../cifar/cifar10_estimator/cifar10_main.py --data-dir=../cifar/cifar10_estimator/cifar-10-data \\\n",
    "                       --job-dir=/tmp/cifar10 \\\n",
    "                       --num-gpus=0 \\\n",
    "                       --train-steps=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'tensorboard.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "%run tensorboard --log-dir=\"/tmp/cifar10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Requested GPUs but none found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/git/cifar/cifar10_estimator/cifar10_main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_gpus\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_gpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Requested GPUs but none found.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_gpus\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     raise ValueError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Requested GPUs but none found."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 0.04605412483215332s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "K.clear_session()\n",
    "t0 = time.time()\n",
    "\n",
    "%run ../cifar/cifar10_estimator/cifar10_main.py --data-dir=../cifar/cifar10_estimator/cifar-10-data \\\n",
    "                       --job-dir=/tmp/cifar10 \\\n",
    "                       --num-gpus=2 \\\n",
    "                       --train-steps=3\n",
    "\n",
    "t1 = time.time()\n",
    "print('time : {}s'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ローカル環境ではGPUはないため、上記は実行できない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 異なるフレームワークへの書き換え\n",
    "\n",
    "「ディープラーニングフレームワーク1」で作成した4種類のデータセットを扱うTensorFLowのコードを異なるフレームワークに変更していきます。KerasはTensorFLowに含まれるtf.kerasモジュールを使用していく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris（２値分類）をKerasで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4)\n",
      "(64, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequentialクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               1000      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 21,201\n",
      "Trainable params: 21,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(200, activation=tf.nn.relu, input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(100, activation=tf.nn.relu, input_shape=(200,)),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "                      metrics=[\"accuracy\"]\n",
    "             )\n",
    "\n",
    "history1  = model1.fit(X_train, y_train, batch_size=5, epochs=1000, verbose=0\n",
    "                    )\n",
    "#validation_data=(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4314786493778229\n",
      "Train accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "score = model1.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session( )\n",
    "\n",
    "#入力から出力までの流れを記述\n",
    "input_data = tf.keras.layers.Input(shape=(4,))\n",
    "output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(input_data)\n",
    "\n",
    "#Modelクラスに渡す\n",
    "model2 = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history2 = model2.fit(X_train, y_train,\n",
    "                    batch_size=5,\n",
    "                    epochs=500,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.182043194770813\n",
      "Train accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "score = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris（多値分類）をKerasで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(dataset_path)\n",
    "y = df3[\"Species\"]\n",
    "X = df3.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y[y=='Iris-setosa'] = 2\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_one_hot = enc.fit_transform(y[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (96, 4)\n",
      "y_train (96, 3)\n",
      "X_val (24, 4)\n",
      "y_val (24, 3)\n",
      "X_test (30, 4)\n",
      "y_test (30, 3)\n"
     ]
    }
   ],
   "source": [
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"X_val\", X_val.shape)\n",
    "print(\"y_val\", y_val.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                250       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 8,473\n",
      "Trainable params: 8,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session( )\n",
    "\n",
    "#入力から出力の流れを記述\n",
    "input_data = tf.keras.layers.Input(shape=(4,))\n",
    "x = tf.keras.layers.Dense(50, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(100, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(30, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(x)\n",
    "model3 = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "#Modelクラスに渡す\n",
    "model3 = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history3 = model3.fit(X_train, y_train,\n",
    "                    batch_size=10,\n",
    "                    epochs=1000,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02233959548175335\n",
      "Train accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = model3.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House PriceをKerasで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1460 entries, 1 to 1460\n",
      "Data columns (total 3 columns):\n",
      "SalePrice    1460 non-null int64\n",
      "GrLivArea    1460 non-null int64\n",
      "YearBuilt    1460 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 45.6 KB\n"
     ]
    }
   ],
   "source": [
    "#データ\n",
    "df_0 = pd.read_csv('train.csv', index_col=0)\n",
    "df = df_0.loc[:,[\"SalePrice\", \"GrLivArea\", \"YearBuilt\"]]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (876, 2)\n",
      "y_train (876, 1)\n",
      "X_val (219, 2)\n",
      "y_val (219, 1)\n",
      "X_test (365, 2)\n",
      "y_test (365, 1)\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:,[1,2]].values\n",
    "y = df[\"SalePrice\"].values.reshape(-1,1)\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.25, random_state=0,)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"X_val\", X_val.shape)\n",
    "print(\"y_val\", y_val.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                150       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 8,311\n",
      "Trainable params: 8,311\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session( )\n",
    "\n",
    "#入力から出力の流れを記述\n",
    "input_data = tf.keras.layers.Input(shape=(2,))\n",
    "x = tf.keras.layers.Dense(50, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(100, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(30, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "model4 = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "#Modelクラスに渡す\n",
    "model4 = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 876 samples, validate on 219 samples\n",
      "Epoch 1/100\n",
      "876/876 [==============================] - 1s 614us/sample - loss: 2918485324.2009 - val_loss: 3187213067.6895\n",
      "Epoch 2/100\n",
      "876/876 [==============================] - 0s 264us/sample - loss: 2899388853.4429 - val_loss: 3149557999.9269\n",
      "Epoch 3/100\n",
      "876/876 [==============================] - 0s 268us/sample - loss: 2932157440.3653 - val_loss: 3157430540.2740\n",
      "Epoch 4/100\n",
      "876/876 [==============================] - 0s 254us/sample - loss: 2944002222.1005 - val_loss: 3204974690.0457\n",
      "Epoch 5/100\n",
      "876/876 [==============================] - 0s 254us/sample - loss: 2932090126.9041 - val_loss: 3407319161.2785\n",
      "Epoch 6/100\n",
      "876/876 [==============================] - 0s 397us/sample - loss: 2922593417.6438 - val_loss: 3255529516.1279\n",
      "Epoch 7/100\n",
      "876/876 [==============================] - 0s 377us/sample - loss: 2926668239.4155 - val_loss: 3194264430.4658\n",
      "Epoch 8/100\n",
      "876/876 [==============================] - 0s 282us/sample - loss: 2945879522.8858 - val_loss: 3212096202.8128\n",
      "Epoch 9/100\n",
      "876/876 [==============================] - 0s 344us/sample - loss: 2907283691.7626 - val_loss: 3160689510.8676\n",
      "Epoch 10/100\n",
      "876/876 [==============================] - 0s 531us/sample - loss: 2952586604.4201 - val_loss: 3330513998.9041\n",
      "Epoch 11/100\n",
      "876/876 [==============================] - 0s 272us/sample - loss: 2922397140.4566 - val_loss: 3266823405.8813\n",
      "Epoch 12/100\n",
      "876/876 [==============================] - 0s 256us/sample - loss: 2914134448.7306 - val_loss: 3248371055.0502\n",
      "Epoch 13/100\n",
      "876/876 [==============================] - 0s 278us/sample - loss: 2938212858.8858 - val_loss: 3195034026.6667\n",
      "Epoch 14/100\n",
      "876/876 [==============================] - 0s 250us/sample - loss: 2921253442.0457 - val_loss: 3298110752.4384\n",
      "Epoch 15/100\n",
      "876/876 [==============================] - 0s 255us/sample - loss: 2911184015.1963 - val_loss: 3161936556.4201\n",
      "Epoch 16/100\n",
      "876/876 [==============================] - 0s 268us/sample - loss: 2935147361.7534 - val_loss: 3196436620.8584\n",
      "Epoch 17/100\n",
      "876/876 [==============================] - 0s 278us/sample - loss: 2934909696.0365 - val_loss: 3166802234.7397\n",
      "Epoch 18/100\n",
      "876/876 [==============================] - 0s 269us/sample - loss: 2924671744.5845 - val_loss: 3349069841.2420\n",
      "Epoch 19/100\n",
      "876/876 [==============================] - 0s 274us/sample - loss: 2942886191.4886 - val_loss: 3202532944.6575\n",
      "Epoch 20/100\n",
      "876/876 [==============================] - 0s 276us/sample - loss: 2931469843.5068 - val_loss: 3237639945.3516\n",
      "Epoch 21/100\n",
      "876/876 [==============================] - 0s 273us/sample - loss: 2928301197.6621 - val_loss: 3213200005.2603\n",
      "Epoch 22/100\n",
      "876/876 [==============================] - 0s 274us/sample - loss: 2915909493.9909 - val_loss: 3160333511.0137\n",
      "Epoch 23/100\n",
      "876/876 [==============================] - 0s 269us/sample - loss: 2920204580.1644 - val_loss: 3388808965.8447\n",
      "Epoch 24/100\n",
      "876/876 [==============================] - 0s 275us/sample - loss: 2921326237.7352 - val_loss: 3217673144.9863\n",
      "Epoch 25/100\n",
      "876/876 [==============================] - 0s 274us/sample - loss: 2920903054.3927 - val_loss: 3222182356.7489\n",
      "Epoch 26/100\n",
      "876/876 [==============================] - 0s 272us/sample - loss: 2903361659.3242 - val_loss: 3383199191.3790\n",
      "Epoch 27/100\n",
      "876/876 [==============================] - 0s 270us/sample - loss: 2956216075.3973 - val_loss: 3340756123.7626\n",
      "Epoch 28/100\n",
      "876/876 [==============================] - 0s 281us/sample - loss: 2927174569.3151 - val_loss: 3181727470.4658\n",
      "Epoch 29/100\n",
      "876/876 [==============================] - 0s 312us/sample - loss: 2921509487.4886 - val_loss: 3256689585.9726\n",
      "Epoch 30/100\n",
      "876/876 [==============================] - 0s 261us/sample - loss: 2904620310.8676 - val_loss: 3512283020.5662\n",
      "Epoch 31/100\n",
      "876/876 [==============================] - 0s 294us/sample - loss: 2916412840.6941 - val_loss: 3387348114.9954\n",
      "Epoch 32/100\n",
      "876/876 [==============================] - 0s 257us/sample - loss: 2949774692.9680 - val_loss: 3190707032.8402\n",
      "Epoch 33/100\n",
      "876/876 [==============================] - 0s 269us/sample - loss: 2969439015.8904 - val_loss: 3215744668.9315\n",
      "Epoch 34/100\n",
      "876/876 [==============================] - 0s 257us/sample - loss: 2912070035.2146 - val_loss: 3166823931.9087\n",
      "Epoch 35/100\n",
      "876/876 [==============================] - 0s 292us/sample - loss: 2925145948.3470 - val_loss: 3165717940.6027\n",
      "Epoch 36/100\n",
      "876/876 [==============================] - 0s 344us/sample - loss: 2914658018.9224 - val_loss: 3215004100.3836\n",
      "Epoch 37/100\n",
      "876/876 [==============================] - 0s 280us/sample - loss: 2942491429.4795 - val_loss: 3165692862.2466\n",
      "Epoch 38/100\n",
      "876/876 [==============================] - 0s 286us/sample - loss: 2936917732.2374 - val_loss: 3376964113.8265\n",
      "Epoch 39/100\n",
      "876/876 [==============================] - 0s 320us/sample - loss: 2899380055.5982 - val_loss: 3165608641.4612\n",
      "Epoch 40/100\n",
      "876/876 [==============================] - 0s 301us/sample - loss: 2954916606.4658 - val_loss: 3182897748.1644\n",
      "Epoch 41/100\n",
      "876/876 [==============================] - 0s 300us/sample - loss: 2930193923.2146 - val_loss: 3154331782.4292\n",
      "Epoch 42/100\n",
      "876/876 [==============================] - 0s 282us/sample - loss: 2925985899.7626 - val_loss: 3172696749.5890\n",
      "Epoch 43/100\n",
      "876/876 [==============================] - 0s 307us/sample - loss: 2907475775.8539 - val_loss: 3309099069.3699\n",
      "Epoch 44/100\n",
      "876/876 [==============================] - 0s 279us/sample - loss: 2932966480.8767 - val_loss: 3182561215.4155\n",
      "Epoch 45/100\n",
      "876/876 [==============================] - 0s 274us/sample - loss: 2942253568.7306 - val_loss: 3173608395.3973\n",
      "Epoch 46/100\n",
      "876/876 [==============================] - 0s 316us/sample - loss: 2937297628.7489 - val_loss: 3315245726.9772\n",
      "Epoch 47/100\n",
      "876/876 [==============================] - 0s 290us/sample - loss: 2926246045.8082 - val_loss: 3168941048.9863\n",
      "Epoch 48/100\n",
      "876/876 [==============================] - 0s 289us/sample - loss: 2928988658.3744 - val_loss: 3174478707.7260\n",
      "Epoch 49/100\n",
      "876/876 [==============================] - 0s 273us/sample - loss: 2926129567.3790 - val_loss: 3273939281.8265\n",
      "Epoch 50/100\n",
      "876/876 [==============================] - 0s 277us/sample - loss: 2904797128.4749 - val_loss: 3238570783.8539\n",
      "Epoch 51/100\n",
      "876/876 [==============================] - 0s 279us/sample - loss: 2935490232.9863 - val_loss: 3218120161.6073\n",
      "Epoch 52/100\n",
      "876/876 [==============================] - 0s 280us/sample - loss: 2918495638.4292 - val_loss: 3188073741.1507\n",
      "Epoch 53/100\n",
      "876/876 [==============================] - 0s 292us/sample - loss: 2911645598.4658 - val_loss: 3328442003.8721\n",
      "Epoch 54/100\n",
      "876/876 [==============================] - 0s 288us/sample - loss: 2923333150.3562 - val_loss: 3275142868.1644\n",
      "Epoch 55/100\n",
      "876/876 [==============================] - 0s 339us/sample - loss: 2940254195.9452 - val_loss: 3274669102.1735\n",
      "Epoch 56/100\n",
      "876/876 [==============================] - 0s 283us/sample - loss: 2945796193.4612 - val_loss: 3297557062.2831\n",
      "Epoch 57/100\n",
      "876/876 [==============================] - 0s 281us/sample - loss: 2929328579.5799 - val_loss: 3283520113.9726\n",
      "Epoch 58/100\n",
      "876/876 [==============================] - 0s 285us/sample - loss: 2926978115.0685 - val_loss: 3315219986.9954\n",
      "Epoch 59/100\n",
      "876/876 [==============================] - 0s 292us/sample - loss: 2955299967.4155 - val_loss: 3378878443.8356\n",
      "Epoch 60/100\n",
      "876/876 [==============================] - 0s 288us/sample - loss: 2935379347.2146 - val_loss: 3270320682.9589\n",
      "Epoch 61/100\n",
      "876/876 [==============================] - 0s 280us/sample - loss: 2920034543.6712 - val_loss: 3333303540.3105\n",
      "Epoch 62/100\n",
      "876/876 [==============================] - 0s 297us/sample - loss: 2927001975.8904 - val_loss: 3217989536.7306\n",
      "Epoch 63/100\n",
      "876/876 [==============================] - 0s 290us/sample - loss: 2964087675.1050 - val_loss: 3250930649.4247\n",
      "Epoch 64/100\n",
      "876/876 [==============================] - 0s 294us/sample - loss: 2913554936.9132 - val_loss: 3205137233.3881\n",
      "Epoch 65/100\n",
      "876/876 [==============================] - 0s 285us/sample - loss: 2941079520.0731 - val_loss: 3509758640.8037\n",
      "Epoch 66/100\n",
      "876/876 [==============================] - 0s 295us/sample - loss: 2940425801.2055 - val_loss: 3388431793.9726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100\n",
      "876/876 [==============================] - 0s 264us/sample - loss: 2927237649.8265 - val_loss: 3183269770.3744\n",
      "Epoch 68/100\n",
      "876/876 [==============================] - 0s 244us/sample - loss: 2917168413.5160 - val_loss: 3178655007.5616\n",
      "Epoch 69/100\n",
      "876/876 [==============================] - 0s 239us/sample - loss: 2937177855.8539 - val_loss: 3319465728.5114\n",
      "Epoch 70/100\n",
      "876/876 [==============================] - 0s 231us/sample - loss: 2926229155.5068 - val_loss: 3206133006.9041\n",
      "Epoch 71/100\n",
      "876/876 [==============================] - 0s 239us/sample - loss: 2919735021.4429 - val_loss: 3187125989.4064\n",
      "Epoch 72/100\n",
      "876/876 [==============================] - 0s 241us/sample - loss: 2953545170.4840 - val_loss: 3252138473.7900\n",
      "Epoch 73/100\n",
      "876/876 [==============================] - 0s 245us/sample - loss: 2931667111.3790 - val_loss: 3176514104.8402\n",
      "Epoch 74/100\n",
      "876/876 [==============================] - 0s 304us/sample - loss: 2907290456.4749 - val_loss: 3169157799.4521\n",
      "Epoch 75/100\n",
      "876/876 [==============================] - 0s 321us/sample - loss: 2913548806.7945 - val_loss: 3329919408.2192\n",
      "Epoch 76/100\n",
      "876/876 [==============================] - 0s 311us/sample - loss: 2922966827.2877 - val_loss: 3320812040.7671\n",
      "Epoch 77/100\n",
      "876/876 [==============================] - 0s 309us/sample - loss: 2941901345.2968 - val_loss: 3174358308.8219\n",
      "Epoch 78/100\n",
      "876/876 [==============================] - 0s 345us/sample - loss: 2955123849.4977 - val_loss: 3409336321.1689\n",
      "Epoch 79/100\n",
      "876/876 [==============================] - 0s 324us/sample - loss: 2939387049.0959 - val_loss: 3247052499.4338\n",
      "Epoch 80/100\n",
      "876/876 [==============================] - 0s 314us/sample - loss: 2924778469.8447 - val_loss: 3303358064.5114\n",
      "Epoch 81/100\n",
      "876/876 [==============================] - 0s 245us/sample - loss: 2909737565.0776 - val_loss: 3217093543.7443\n",
      "Epoch 82/100\n",
      "876/876 [==============================] - 0s 313us/sample - loss: 2914160922.1553 - val_loss: 3282776372.8950\n",
      "Epoch 83/100\n",
      "876/876 [==============================] - 0s 287us/sample - loss: 2917195880.1096 - val_loss: 3252064466.9954\n",
      "Epoch 84/100\n",
      "876/876 [==============================] - 0s 269us/sample - loss: 2935009942.3562 - val_loss: 3303469515.1050\n",
      "Epoch 85/100\n",
      "876/876 [==============================] - 0s 261us/sample - loss: 2939713105.0594 - val_loss: 3168649258.6667\n",
      "Epoch 86/100\n",
      "876/876 [==============================] - 0s 271us/sample - loss: 2946375723.7626 - val_loss: 3230233614.6119\n",
      "Epoch 87/100\n",
      "876/876 [==============================] - 0s 261us/sample - loss: 2899122998.7215 - val_loss: 3266580358.7215\n",
      "Epoch 88/100\n",
      "876/876 [==============================] - 0s 259us/sample - loss: 2918540114.4475 - val_loss: 3341994221.8813\n",
      "Epoch 89/100\n",
      "876/876 [==============================] - 0s 255us/sample - loss: 2941808889.0594 - val_loss: 3320012606.5388\n",
      "Epoch 90/100\n",
      "876/876 [==============================] - 0s 244us/sample - loss: 2913553636.3836 - val_loss: 3180260609.1689\n",
      "Epoch 91/100\n",
      "876/876 [==============================] - 0s 253us/sample - loss: 2928595871.7078 - val_loss: 3219933598.9772\n",
      "Epoch 92/100\n",
      "876/876 [==============================] - 0s 250us/sample - loss: 2942664981.1507 - val_loss: 3301177528.1096\n",
      "Epoch 93/100\n",
      "876/876 [==============================] - 0s 250us/sample - loss: 2939569917.5890 - val_loss: 3299141769.6438\n",
      "Epoch 94/100\n",
      "876/876 [==============================] - 0s 253us/sample - loss: 2946569932.2740 - val_loss: 3265246657.7534\n",
      "Epoch 95/100\n",
      "876/876 [==============================] - 0s 249us/sample - loss: 2920961865.1689 - val_loss: 3182458114.9224\n",
      "Epoch 96/100\n",
      "876/876 [==============================] - 0s 287us/sample - loss: 2898280391.5982 - val_loss: 3283700184.8402\n",
      "Epoch 97/100\n",
      "876/876 [==============================] - 0s 345us/sample - loss: 2909904230.2831 - val_loss: 3231758569.4977\n",
      "Epoch 98/100\n",
      "876/876 [==============================] - 0s 251us/sample - loss: 2953285307.7626 - val_loss: 3235275367.4521\n",
      "Epoch 99/100\n",
      "876/876 [==============================] - 0s 250us/sample - loss: 2921084126.3562 - val_loss: 3270583552.5845\n",
      "Epoch 100/100\n",
      "876/876 [==============================] - 0s 247us/sample - loss: 2934532454.6484 - val_loss: 3185664369.3881\n"
     ]
    }
   ],
   "source": [
    "model4.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "              )\n",
    "history4 = model4.fit(X_train, y_train,\n",
    "                    batch_size=10,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2906197760.584475\n",
      "val    loss: 3185664381.0776258\n",
      "test  loss: 3473686276.909589\n"
     ]
    }
   ],
   "source": [
    "score = model4.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"train loss:\", score)\n",
    "score = model4.evaluate(X_val, y_val, verbose=0)\n",
    "print(\"val    loss:\", score)\n",
    "score = model4.evaluate(X_test, y_test, verbose=0)\n",
    "print('test  loss:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNISTをKerasで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xt_train (48000, 28, 28, 1)\n",
      "Xt_val  (12000, 28, 28, 1)\n",
      "yt_train (48000, 10)\n",
      "yt_val  (12000, 10)\n",
      "Xt_test (10000, 28, 28, 1)\n",
      "y_test (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#MNISTデータセット\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#平滑化\n",
    "#X_train_f = X_train.reshape(-1, 784)\n",
    "#X_test_f = X_test.reshape(-1, 784)\n",
    "\n",
    "#前処理\n",
    "X_train_ff = X_train.astype(np.float)\n",
    "X_test_ff = X_test.astype(np.float)\n",
    "X_train_ff /= 255\n",
    "X_test_ff /= 255\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "Xt_train, Xt_val, yt_train, yt_val = train_test_split(X_train_ff, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "Xt_train = Xt_train.reshape(-1, 28, 28, 1)\n",
    "Xt_val = Xt_val.reshape(-1, 28, 28, 1)\n",
    "Xt_test = X_test_ff.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(\"Xt_train\", Xt_train.shape) # (48000, 784)\n",
    "print(\"Xt_val \", Xt_val.shape) # (12000, 784)\n",
    "print(\"yt_train\", yt_train.shape)\n",
    "print(\"yt_val \", yt_val.shape)\n",
    "print(\"Xt_test\", Xt_test.shape)\n",
    "print(\"y_test\", y_test_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "K.clear_session( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "model5 = Sequential()\n",
    "\n",
    "model5.add(Conv2D(32,(3, 3),input_shape=(28,28,1)))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model5.add(Conv2D(64,(3,3)))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model5.add(Conv2D(64,(3,3)))\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(64))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dense(10))\n",
    "model5.add(Activation('softmax'))\n",
    "\n",
    "model5.compile(optimizer=\"rmsprop\", loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishizucat/.pyenv/versions/anaconda3-5.3.1/envs/term3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 513s 11ms/step - loss: 0.0695 - acc: 0.9788 - val_loss: 0.0576 - val_acc: 0.9849\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 497s 10ms/step - loss: 0.0473 - acc: 0.9867 - val_loss: 0.0523 - val_acc: 0.9846\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 500s 10ms/step - loss: 0.0388 - acc: 0.9891 - val_loss: 0.0464 - val_acc: 0.9879\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 503s 10ms/step - loss: 0.0341 - acc: 0.9915 - val_loss: 0.0474 - val_acc: 0.9875\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 496s 10ms/step - loss: 0.0305 - acc: 0.9919 - val_loss: 0.0489 - val_acc: 0.9895\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 512s 11ms/step - loss: 0.0307 - acc: 0.9924 - val_loss: 0.0689 - val_acc: 0.9864\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 561s 12ms/step - loss: 0.0302 - acc: 0.9940 - val_loss: 0.0799 - val_acc: 0.9888\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 520s 11ms/step - loss: 0.0279 - acc: 0.9937 - val_loss: 0.0795 - val_acc: 0.9879\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 499s 10ms/step - loss: 0.0258 - acc: 0.9942 - val_loss: 0.0559 - val_acc: 0.9907\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 498s 10ms/step - loss: 0.0296 - acc: 0.9943 - val_loss: 0.0876 - val_acc: 0.9871\n"
     ]
    }
   ],
   "source": [
    "history5 = model5.fit(Xt_train, yt_train, batch_size=20, nb_epoch=10, verbose=1, validation_data=(Xt_val, yt_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06916410112185609\n",
      "Train accuracy: 0.9888\n"
     ]
    }
   ],
   "source": [
    "score = model5.evaluate(Xt_test, y_test_one_hot, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorchへの書き換え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #基本モジュール\n",
    "from torch.autograd import Variable #自動微分用\n",
    "import torch.nn as nn #ネットワーク構築用\n",
    "import torch.optim as optim #最適化関数\n",
    "import torch.nn.functional as F #ネットワーク用の様々な関数\n",
    "import torch.utils.data #データセット読み込み関連\n",
    "import torchvision #画像関連\n",
    "from torchvision import datasets, models, transforms #画像用データセット諸々"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4)\n",
      "(64, 1)\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考：http://aidiary.hatenablog.com/entry/20180203/1517629555\n",
    "\n",
    "input_size = 4\n",
    "num_classes = 3\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.01\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model6 = LogisticRegression(input_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model6.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 2.9394 val_loss: 2.0976 val_acc: 0.0000\n",
      "epoch 1000, loss: 0.3186 val_loss: 0.3623 val_acc: 0.9000\n",
      "epoch 2000, loss: 0.2291 val_loss: 0.2920 val_acc: 0.9000\n",
      "epoch 3000, loss: 0.1882 val_loss: 0.2651 val_acc: 0.9000\n",
      "epoch 4000, loss: 0.1642 val_loss: 0.2522 val_acc: 0.9000\n",
      "epoch 5000, loss: 0.1482 val_loss: 0.2452 val_acc: 0.9000\n",
      "epoch 6000, loss: 0.1367 val_loss: 0.2412 val_acc: 0.9000\n",
      "epoch 7000, loss: 0.1278 val_loss: 0.2390 val_acc: 0.9000\n",
      "epoch 8000, loss: 0.1208 val_loss: 0.2378 val_acc: 0.9000\n",
      "epoch 9000, loss: 0.1151 val_loss: 0.2374 val_acc: 0.9000\n"
     ]
    }
   ],
   "source": [
    "def train(X_train, y_train):\n",
    "    inputs = torch.from_numpy(X_train).float()\n",
    "    targets = torch.from_numpy(y_train).long()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model6(inputs)\n",
    "    \n",
    "    targets = targets.squeeze_()\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def valid(X_test, y_test):\n",
    "    inputs = torch.from_numpy(X_test).float()\n",
    "    targets = torch.from_numpy(y_test).long()\n",
    "\n",
    "    targets = targets.squeeze_()\n",
    "    outputs = model6(inputs)\n",
    "    val_loss = criterion(outputs, targets)\n",
    "    \n",
    "    # 精度を求める\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == targets).sum().item()\n",
    "    val_acc = float(correct) / targets.size(0)\n",
    "\n",
    "    return val_loss.item(), val_acc\n",
    "\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    perm = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(perm)\n",
    "    X_train = X_train[perm]\n",
    "    y_train = y_train[perm]\n",
    "    \n",
    "    loss = train(X_train, y_train)\n",
    "    val_loss, val_acc = valid(X_test, y_test)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print('epoch %d, loss: %.4f val_loss: %.4f val_acc: %.4f'\n",
    "              % (epoch, loss, val_loss, val_acc))\n",
    "    \n",
    "    # logging\n",
    "    loss_list.append(loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot learning curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), loss_list, 'r-', label='train_loss')\n",
    "plt.plot(range(num_epochs), val_loss_list, 'b-', label='val_loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), val_acc_list, 'g-', label='val_acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multi-target not supportedエラーについて\n",
    "\n",
    "＜参考＞\n",
    "https://stackoverflow.com/questions/49206550/pytorch-error-multi-target-not-supported-in-crossentropyloss/49209628\n",
    "\n",
    "loss関数にyラベルを渡す時、(batchSize x 1)のshapeになるが、これは(batchSize, )のtensorとして認識されない。loss関数に渡す時は、後者の形にする必要があり、この時、squeeze関数を使用すると良い。squeeze()は大きさが1の次元を全て削除した配列を返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NumpyをTensorへ変換、データセット準備\n",
    "train = torch.utils.data.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train))\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=20, shuffle=True)\n",
    "test = torch.utils.data.TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test))\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at /Users/distiller/project/conda/conda-bld/pytorch_1573049287641/work/aten/src/THNN/generic/ClassNLLCriterion.c:22",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-d792665b5ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/term3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/term3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/term3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/term3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: multi-target not supported at /Users/distiller/project/conda/conda-bld/pytorch_1573049287641/work/aten/src/THNN/generic/ClassNLLCriterion.c:22"
     ]
    }
   ],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(4,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "#モデル定義\n",
    "model6 = LogisticRegression()\n",
    "\n",
    "#lossとoptimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model6.parameters(), lr=0.01)\n",
    "\n",
    "#エポック数の指定\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i , data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        #labels = labels.squeeze_()\n",
    "        \n",
    "        outputs = model6(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        print('[%d, %5d] loss: %.f3' %(epoch + 1, i + 1, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フレームワークの比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **theano**\n",
    "\n",
    "DLフレームワークの祖父。\n",
    "\n",
    "  - メリット\n",
    "    - 計算グラフは見やすく抽象化されている\n",
    "    - 再帰型ニューラルネットワークは計算グラフに良く適合する\n",
    "    - 高レベルのラッパー（Keras、Lasagne）により、手間が省ける\n",
    "  \n",
    "  - デメリット\n",
    "    - エラーメッセージが役に立たないことがある\n",
    "    - 大きなモデルの場合、コンパイルの時間が長くなることがある\n",
    "    - 事前トレーニングを受けたモデルへのサポートが不十分\n",
    "    - 単一GPU\n",
    "\n",
    "\n",
    "2. **TensorFlow**\n",
    "\n",
    "Googleが開発しているライブラリ。C++で作られているが、Pythonも含まれており柔軟に使用できる構造になっている。\n",
    "\n",
    "  - メリット\n",
    "    - 計算グラフの抽象化\n",
    "    - Theanoよりもコンパイル時間が短い\n",
    "    - TensorBoardを利用して視覚化が可能\n",
    "    - データとモデルの並列処理\n",
    "    \n",
    "  - デメリット\n",
    "    - 他のフレームワークより遅い\n",
    "    - 事前トレーニングされたモデルがあまりない。\n",
    "    - 計算グラフは単なるPythonなため、遅い。\n",
    "    - 商用サポートなし。\n",
    "    - 新しいトレーニングのバッチがあるごとに読み込みをするためにPythonにドロップアウトする\n",
    "    - 大規模なソフトウェアプロジェクトで動的型付けにエラーが発生しやすい\n",
    "\n",
    "\n",
    "3. **Caffe**\n",
    "\n",
    "画像処理を主としたライブラリ。\n",
    "\n",
    "  - メリット\n",
    "    - フィードフォワードネットワークと、画像処理に適している。\n",
    "    - 既存のネットワークの微調整に適している\n",
    "    - コードを書かずにモデルをトレーニングできる\n",
    "    \n",
    "  - デメリット\n",
    "    - 新しいGPU層にC++/CUDAを書く必要がある。\n",
    "    - 再帰型ニューラルネットワークには適していない。\n",
    "    - 大規模なネットワーク（GoogLeNet, ResNet）には使いにくい\n",
    "    - 拡張性がない\n",
    "    - 商用サポートがない\n",
    "    - 開発に時間がかかる\n",
    "\n",
    "\n",
    "4. **Chainer**\n",
    "\n",
    "日本で開発されていたライブラリ。2019年12月に開発終了。動的計算グラフのフレームワーク。自然言語処理には人気がある。\n",
    "  - メリット\n",
    "    - 直感的な計算グラフの構築が可能\n",
    "    - デバッグが比較的行いやすい\n",
    "    \n",
    "  - デメリット\n",
    "    - 計算速度が遅くなりがち\n",
    "\n",
    "\n",
    "5. **Keras**\n",
    "\n",
    "直感的なAPI、高速計算ライブラリのラッパー\n",
    "\n",
    "  - メリット\n",
    "    - 扱いやすい\n",
    "    - 利用者も多い\n",
    "    \n",
    "  - デメリット\n",
    "    - 処理の中身はコードからは全くわからない\n",
    "    - オリジナルの処理をさせるのが面倒\n",
    "    - 計算グラフ構築後、変更不可能\n",
    "    \n",
    "6. **pytorch**\n",
    "\n",
    "chainerに近い構文構造。defined by run。\n",
    "\n",
    "  - メリット\n",
    "    - 実行速度が速い\n",
    "    - 比較的自由に記述できる\n",
    "    \n",
    "  - デメリット\n",
    "    - バッチ処理前提\n",
    "    - 記述が煩雑になりがち"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "252.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
